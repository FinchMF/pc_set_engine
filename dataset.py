"""Dataset utilities for working with music generation datasets.

This module provides tools for loading, processing, and analyzing datasets
created by the Monte Carlo simulation framework. It includes functionality
for handling MIDI files, extracting features, and preparing data for
machine learning applications.

Examples:
    Loading a dataset and extracting features:
    ```python
    from dataset import DatasetManager
    
    dataset = DatasetManager("path/to/dataset")
    features_df = dataset.get_features_dataframe()
    ```
"""
import os
import json
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Union, Optional, Tuple
import mido
import matplotlib.pyplot as plt
import seaborn as sns

from utils import get_logger

logger = get_logger(__name__)

class DatasetManager:
    """Manager for working with music generation datasets.
    
    This class provides tools for loading, processing, and analyzing
    datasets generated by the Monte Carlo simulator.
    
    Attributes:
        dataset_path: Path to the dataset directory
        metadata: Dataset metadata
        samples: List of dataset samples
        features_df: DataFrame of feature vectors
    """
    
    def __init__(self, dataset_path: str):
        """Initialize the dataset manager.
        
        Args:
            dataset_path: Path to the dataset directory
        """
        self.dataset_path = Path(dataset_path)
        self.metadata_path = self.dataset_path / "dataset.json"
        
        # Load dataset metadata
        if not self.metadata_path.exists():
            raise FileNotFoundError(f"Dataset metadata not found at {self.metadata_path}")
        
        with open(self.metadata_path, 'r') as f:
            dataset = json.load(f)
        
        self.metadata = dataset["metadata"]
        self.samples = dataset["samples"]
        self.features_df = None
        
        logger.info(f"Loaded dataset with {self.metadata['num_samples']} samples")
    
    def get_features_dataframe(self) -> pd.DataFrame:
        """Get a pandas DataFrame containing all feature vectors.
        
        Returns:
            DataFrame with feature vectors
        """
        if self.features_df is not None:
            return self.features_df
        
        # Extract features from all samples
        data = []
        for sample in self.samples:
            row = {
                "midi_file": sample["midi_file"],
                "id": sample["id"]
            }
            row.update(sample["features"])
            data.append(row)
        
        # Create DataFrame
        self.features_df = pd.DataFrame(data)
        return self.features_df
    
    def get_midi_path(self, sample_id: Union[int, str]) -> Path:
        """Get the path to a MIDI file for a specific sample.
        
        Args:
            sample_id: Sample ID or filename
            
        Returns:
            Path to the MIDI file
        """
        # Handle both ID and filename
        if isinstance(sample_id, int):
            # Find sample by ID
            for sample in self.samples:
                if sample["id"] == sample_id:
                    midi_file = sample["midi_file"]
                    break
            else:
                raise ValueError(f"Sample with ID {sample_id} not found")
        else:
            # Assume it's a filename
            midi_file = sample_id
        
        return self.dataset_path / midi_file
    
    def analyze_parameter_distributions(self):
        """Analyze and visualize the distribution of parameters in the dataset."""
        df = self.get_features_dataframe()
        
        # Find numeric columns for analysis
        numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
        
        # Create distribution plots
        n_cols = 3
        n_rows = (len(numeric_cols) + n_cols - 1) // n_cols
        
        plt.figure(figsize=(15, n_rows * 4))
        
        for i, col in enumerate(numeric_cols, 1):
            plt.subplot(n_rows, n_cols, i)
            sns.histplot(df[col], kde=True)
            plt.title(f"Distribution of {col}")
            plt.tight_layout()
        
        plt.savefig(self.dataset_path / "parameter_distributions.png")
        plt.close()
    
    def analyze_feature_correlations(self):
        """Analyze and visualize correlations between features."""
        df = self.get_features_dataframe()
        
        # Find numeric columns for correlation analysis
        numeric_df = df.select_dtypes(include=['int64', 'float64'])
        
        if len(numeric_df.columns) > 1:
            # Calculate correlation matrix
            corr_matrix = numeric_df.corr()
            
            # Visualize correlations
            plt.figure(figsize=(12, 10))
            sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
            plt.title('Feature Correlations')
            plt.tight_layout()
            plt.savefig(self.dataset_path / "feature_correlations.png")
            plt.close()
    
    def extract_midi_features(self, regenerate: bool = False):
        """Extract musical features directly from MIDI files.
        
        Args:
            regenerate: If True, regenerate features even if they already exist
        """
        # Check if features already exist
        features_path = self.dataset_path / "midi_features.csv"
        if features_path.exists() and not regenerate:
            logger.info(f"MIDI features already exist at {features_path}")
            return features_path
        
        # Extract features from each MIDI file
        midi_features = []
        
        for sample in self.samples:
            midi_path = self.dataset_path / sample["midi_file"]
            
            try:
                # Extract basic MIDI statistics
                midi_file = mido.MidiFile(midi_path)
                
                # Calculate note density, pitch range, etc.
                notes = []
                total_time = 0
                tempo = 500000  # Default tempo (500000 microseconds per beat = 120 BPM)
                
                for track in midi_file.tracks:
                    time = 0
                    for msg in track:
                        time += msg.time
                        
                        # Extract tempo if available
                        if msg.type == 'set_tempo':
                            tempo = msg.tempo
                        
                        # Extract note information
                        if msg.type == 'note_on' and msg.velocity > 0:
                            notes.append((msg.note, time))
                    
                    total_time = max(total_time, time)
                
                # Extract features
                if notes:
                    pitches = [note[0] for note in notes]
                    # Convert ticks to seconds based on tempo
                    seconds_per_tick = tempo / (midi_file.ticks_per_beat * 1000000)
                    duration_seconds = total_time * seconds_per_tick
                    note_density = len(notes) / duration_seconds if duration_seconds > 0 else 0
                    
                    # Add to features
                    midi_features.append({
                        "id": sample["id"],
                        "midi_file": sample["midi_file"],
                        "note_count": len(notes),
                        "note_density": note_density,
                        "pitch_range": max(pitches) - min(pitches),
                        "mean_pitch": sum(pitches) / len(pitches),
                        "min_pitch": min(pitches),
                        "max_pitch": max(pitches),
                        "duration_seconds": duration_seconds,
                        "tempo_bpm": 60000000 / tempo  # Convert microseconds per beat to BPM
                    })
                    
                    logger.debug(f"Successfully extracted features from {midi_path}")
                else:
                    logger.warning(f"No notes found in {midi_path}")
            
            except Exception as e:
                logger.error(f"Error extracting features from {midi_path}: {e}")
        
        # Save features to CSV
        if midi_features:
            features_df = pd.DataFrame(midi_features)
            features_df.to_csv(features_path, index=False)
            logger.info(f"Extracted MIDI features saved to {features_path}")
            return features_path
        
        logger.warning("No MIDI features could be extracted")
        return None


def create_dataset_from_directories(input_dirs: List[str], output_dir: str) -> str:
    """Create a combined dataset from multiple Monte Carlo simulation directories.
    
    Args:
        input_dirs: List of paths to Monte Carlo simulation output directories
        output_dir: Path where the combined dataset should be saved
        
    Returns:
        Path to the generated dataset metadata file
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # Create dataset structure
    dataset = {
        "metadata": {
            "generated_at": datetime.datetime.now().isoformat(),
            "num_samples": 0,
            "source_directories": input_dirs,
            "parameter_space": {},
            "feature_types": {}
        },
        "samples": []
    }
    
    # Process each input directory
    for input_dir in input_dirs:
        input_path = Path(input_dir)
        json_files = list(input_path.glob("*.json"))
        
        for json_file in json_files:
            if json_file.name == "dataset.json":
                # This is already a dataset file, import its samples
                with open(json_file, 'r') as f:
                    source_dataset = json.load(f)
                
                for sample in source_dataset.get("samples", []):
                    # Copy MIDI file
                    midi_path = input_path / sample["midi_file"]
                    if midi_path.exists():
                        new_midi_name = f"{input_path.name}_{sample['midi_file']}"
                        new_midi_path = Path(output_dir) / new_midi_name
                        
                        import shutil
                        shutil.copy2(midi_path, new_midi_path)
                        
                        # Update sample and add to dataset
                        sample_copy = sample.copy()
                        sample_copy["midi_file"] = new_midi_name
                        sample_copy["source"] = input_path.name
                        dataset["samples"].append(sample_copy)
                        dataset["metadata"]["num_samples"] += 1
            
            elif json_file.name.endswith("_dataset.json") or json_file.name == "monte_carlo_dataset.json":
                # This is a Monte Carlo results file
                with open(json_file, 'r') as f:
                    mc_dataset = json.load(f)
                
                # Process each simulation
                for sim in mc_dataset.get("simulations", []):
                    if not sim.get("success", False) or "midi_file" not in sim or not sim["midi_file"]:
                        continue
                    
                    # Copy MIDI file
                    midi_path = Path(sim["midi_file"])
                    if midi_path.exists():
                        new_midi_name = f"{input_path.name}_{midi_path.name}"
                        new_midi_path = Path(output_dir) / new_midi_name
                        
                        import shutil
                        shutil.copy2(midi_path, new_midi_path)
                        
                        # Create sample entry
                        features = {}
                        
                        # Include generation parameters
                        config = sim["config"]
                        features["generation_type"] = config["generation_type"]
                        features["sequence_length"] = config["sequence_length"]
                        features["randomness_factor"] = config["randomness_factor"]
                        features["variation_probability"] = config["variation_probability"]
                        features["progression"] = config.get("progression", False)
                        features["progression_type"] = config.get("progression_type", "static")
                        
                        # Include rhythm parameters
                        if "rhythm_config" in sim:
                            rhythm = sim["rhythm_config"]
                            for key, value in rhythm.items():
                                if key == "time_signature" and isinstance(value, tuple):
                                    features["rhythm_time_signature_numerator"] = value[0]
                                    features["rhythm_time_signature_denominator"] = value[1]
                                    features["rhythm_time_signature_ratio"] = value[0] / value[1]
                                elif isinstance(value, (int, float, str, bool)):
                                    features[f"rhythm_{key}"] = value
                        
                        # Include interval weights
                        if "interval_weights" in sim:
                            weights = sim["interval_weights"]
                            for interval, weight in weights.items():
                                features[f"weight_{interval}"] = weight
                        
                        # Add to dataset
                        dataset["samples"].append({
                            "midi_file": new_midi_name,
                            "id": f"{input_path.name}_{sim['id']}",
                            "features": features,
                            "statistics": sim.get("statistics", {}),
                            "source": input_path.name
                        })
                        
                        dataset["metadata"]["num_samples"] += 1
    
    # Save dataset metadata
    metadata_path = os.path.join(output_dir, "dataset.json")
    with open(metadata_path, 'w') as f:
        json.dump(dataset, f, indent=2)
    
    # Create CSV version
    csv_path = os.path.join(output_dir, "dataset.csv")
    rows = []
    
    if dataset["samples"]:
        # Get all possible feature names
        feature_names = set()
        for sample in dataset["samples"]:
            feature_names.update(sample["features"].keys())
        
        # Sort feature names for consistency
        feature_names = sorted(list(feature_names))
        
        # Create CSV rows
        for sample in dataset["samples"]:
            row = {
                "midi_file": sample["midi_file"],
                "id": sample["id"],
                "source": sample.get("source", "")
            }
            
            # Add each feature
            for name in feature_names:
                row[name] = sample["features"].get(name, None)
            
            rows.append(row)
        
        # Write to CSV
        with open(csv_path, 'w', newline='') as f:
            fieldnames = ["midi_file", "id", "source"] + feature_names
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            for row in rows:
                writer.writerow(row)
    
    logger.info(f"Created combined dataset with {dataset['metadata']['num_samples']} samples at {output_dir}")
    return metadata_path

if __name__ == "__main__":
    import argparse
    import csv
    import datetime
    import sys
    
    parser = argparse.ArgumentParser(description="Dataset utilities for music generation")
    
    subparsers = parser.add_subparsers(dest="command", help="Command to execute")
    
    # Combine datasets command
    combine_parser = subparsers.add_parser("combine", help="Combine multiple datasets")
    combine_parser.add_argument("--input-dirs", nargs="+", required=True,
                               help="Input directories containing Monte Carlo results")
    combine_parser.add_argument("--output-dir", required=True,
                               help="Output directory for combined dataset")
    
    # Analyze dataset command
    analyze_parser = subparsers.add_parser("analyze", help="Analyze a dataset")
    analyze_parser.add_argument("--dataset-dir", required=True,
                               help="Path to dataset directory")
    analyze_parser.add_argument("--extract-midi-features", action="store_true",
                               help="Extract features directly from MIDI files")
    
    args = parser.parse_args()
    
    if args.command == "combine":
        create_dataset_from_directories(args.input_dirs, args.output_dir)
    
    elif args.command == "analyze":
        try:
            dataset = DatasetManager(args.dataset_dir)
            dataset.analyze_parameter_distributions()
            dataset.analyze_feature_correlations()
            
            if args.extract_midi_features:
                dataset.extract_midi_features()
        except Exception as e:
            logger.error(f"Error analyzing dataset: {e}")
            sys.exit(1)
    
    else:
        parser.print_help()
